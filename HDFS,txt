what is hdfs: 
HDFS holds very large amount of data and provides easier access. To store such huge data, the files are stored across multiple machines. These files are stored in redundant fashion to rescue the system from possible data losses in case of failure. HDFS also makes applications available to parallel processing.

Block
Generally the user data is stored in the files of HDFS. The file in a file system will be divided into one or more segments and/or stored in individual data nodes. These file segments are called as blocks. In other words, the minimum amount of data that HDFS can read or write is called a Block. The default block size is 64MB, but it can be increased as per the need to change in HDFS configuration.


Other software components that can run on top of or alongside Hadoop and have achieved top-level Apache project status include:

Pig – a platform for manipulating data stored in HDFS that includes a compiler for MapReduce programs and a high-level language called Pig Latin. It provides a way to perform data extractions, transformations and loading, and basic analysis without having to write MapReduce programs.

Hive – a data warehousing and SQL-like query language that presents data in the form of tables. Hive programming is similar to database programming. (It was initially developed by Facebook.)

HBase – a nonrelational, distributed database that runs on top of Hadoop. HBase tables can serve as input and output for MapReduce jobs.

HCatalog – a table and storage management layer that helps users share and access data.

Ambari – a web interface for managing, configuring and testing Hadoop services and components.

Cassandra – A distributed database system.

Chukwa – a data collection system for monitoring large distributed systems.

Flume – software that collects, aggregates and moves large amounts of streaming data into HDFS.

Oozie – a Hadoop job scheduler.

Sqoop – a connection and transfer mechanism that moves data between Hadoop and relational databases.

Spark – an open-source cluster computing framework with in-memory analytics.

Solr – an scalable search tool that includes indexing, reliability, central configuration, failover and recovery.

Zookeeper – an application that coordinates distributed processes.
